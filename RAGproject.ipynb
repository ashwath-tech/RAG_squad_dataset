{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RtO5byk0ECp"
      },
      "outputs": [],
      "source": [
        "!{sys.executable} -m pip install langchain\n",
        "!pip install -U langchain-text-splitters\n",
        "!pip install langchain_chroma\n",
        "!pip install langchain-huggingface\n",
        "!pip install -U bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DV0mkxZEzz3k"
      },
      "outputs": [],
      "source": [
        "#imports\n",
        "import torch\n",
        "import os\n",
        "import sys\n",
        "import re\n",
        "import json\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "from langchain_core.documents import Document\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from transformers import BitsAndBytesConfig\n",
        "from huggingface_hub import login\n",
        "from transformers import pipeline\n",
        "from google.colab import userdata\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from langchain_core.messages import SystemMessage, HumanMessage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cjcew1uG4DCA"
      },
      "outputs": [],
      "source": [
        "hf = userdata.get('HF_TOKEN')\n",
        "login(hf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBRbZYnEIpDe"
      },
      "outputs": [],
      "source": [
        "ds = load_dataset(\"squad\")\n",
        "df = ds[\"train\"].to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WarXyYsNLiha"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "eval_data = []\n",
        "\n",
        "for _, row in df.head(500).iterrows():\n",
        "    eval_entry = {\n",
        "        \"question\": row[\"question\"],\n",
        "        \"ground_truth\": row[\"answers\"][\"text\"][0],\n",
        "        \"reference_context\": row[\"context\"]\n",
        "    }\n",
        "    eval_data.append(eval_entry)\n",
        "\n",
        "with open(\"squad_eval.jsonl\", \"w\") as f:\n",
        "    for entry in eval_data:\n",
        "        f.write(json.dumps(entry) + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEcy5CZNE08e"
      },
      "outputs": [],
      "source": [
        "documents = []\n",
        "for _, row in df.iterrows():\n",
        "    text = row[\"context\"]\n",
        "    documents.append(Document(page_content=text, metadata={\"question\": row[\"question\"]}))\n",
        "documents = documents[:10000]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UcYdv2iiaGy"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "    device_map=\"auto\",\n",
        "    quantization_config= quant_config\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8CIdsOQzwuK"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWskmYAl1GTB"
      },
      "outputs": [],
      "source": [
        "#create the embedding model\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\") # model_kwargs={\"device\": \"cuda\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nb3V2Q-Khx-U"
      },
      "outputs": [],
      "source": [
        "#split documentsa to chuhnks which can then be vectorised\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
        "texts = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "3002282a6b224bfc94260d5e2d5e3ace",
            "df33a3d58a4e45189dfbfc0f424164ca",
            "1cd1a23236494d02b8a632c3f3b7fed9",
            "31e8f0e7adcb445599448b84f2a2f257",
            "8a69fa38eae443c9a81dd1383e58fe47",
            "edde3052bdbb47a98740f68267c454ad",
            "5037b8a97f284f40a77efa8d456200e0",
            "9698c0ba2ea2496797430af644a12d07",
            "a4117d3692474dd98415ae6238312956",
            "0e9c4a5c6c36482b8d2e4d64e4ad8152",
            "d961dcb7390a448380cbf203614bcccb"
          ]
        },
        "id": "fH4sN1IctCMz",
        "outputId": "0afed02d-2836-47cd-a403-0e71b0775eeb"
      },
      "outputs": [],
      "source": [
        "evaluation_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\", device_map=\"auto\", quantization_config= quant_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p00wc3gctcmD"
      },
      "outputs": [],
      "source": [
        "comparison_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
        "comparison_tokenizer.pad_token = comparisontokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbm3A5tXzw08"
      },
      "outputs": [],
      "source": [
        "db_name = \"RAG-database\"\n",
        "if os.path.exists(db_name):\n",
        "  Chroma(\n",
        "      persist_directory=db_name,\n",
        "      embedding_function=embedding_model\n",
        "  ).delete_collection()\n",
        "vectordb = Chroma.from_documents(\n",
        "  documents=texts,\n",
        "  embedding=embedding_model,\n",
        "  persist_directory=db_name\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7TReZcCdzwrP"
      },
      "outputs": [],
      "source": [
        "retriever = vectordb.as_retriever(search_kwargs={\"k\": 5})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHCzRcih4XTO"
      },
      "outputs": [],
      "source": [
        "system_prompt = '''You are a question answering system.\n",
        "Answer the user's question directly.\n",
        "Phrase the answers well.\n",
        "Do NOT create multiple-choice questions.\n",
        "Do NOT rephrase the question.\n",
        "Do NOT generalize or paraphraze the answer.\n",
        "Use only the given context.\n",
        "Answer in one short sentence.\n",
        "If not in context, say \"I don't know\".\n",
        "Use the subject given in the question in the answer.\n",
        "Answer with the specific noun phrase only.\n",
        "\n",
        "Context:\n",
        "{related_documents}\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZBP4ZvCj1H4"
      },
      "outputs": [],
      "source": [
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAIcc9t7Uzxi"
      },
      "outputs": [],
      "source": [
        "history = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37kbqFZLbJJH"
      },
      "outputs": [],
      "source": [
        "def rag(question):\n",
        "  global history\n",
        "  alldocs = retriever.invoke(question)\n",
        "  context = \"\\n\".join(doc.page_content for doc in alldocs)\n",
        "  system_prompt_final = system_prompt.format(related_documents = context)\n",
        "\n",
        "  message = [{\"role\":\"system\",\"content\":system_prompt_final}]\n",
        "  message.extend(history)\n",
        "  message.append({\"role\":\"user\",\"content\":question})\n",
        "\n",
        "  prompt = tokenizer.apply_chat_template(\n",
        "      message,tokenize=True,\n",
        "      add_generation_prompt=True,\n",
        "      return_tensors=\"pt\"\n",
        "  ).to(\"cuda\")\n",
        "\n",
        "  output = model.generate(\n",
        "      prompt,do_sample=False,\n",
        "      max_new_tokens = 200\n",
        "  )\n",
        "\n",
        "  input_len = prompt.shape[1]\n",
        "  generated = output[0][prompt.shape[-1]:]\n",
        "\n",
        "  answer = tokenizer.decode(\n",
        "      generated,\n",
        "      skip_special_tokens=True\n",
        "  )\n",
        "\n",
        "  history.append({\"role\": \"user\", \"content\": question})\n",
        "  history.append({\"role\": \"assistant\", \"content\": answer})\n",
        "\n",
        "  return answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Plni7KypmSQ3"
      },
      "outputs": [],
      "source": [
        "read_data = []\n",
        "with open(\"squad_eval.jsonl\", \"r\") as f:\n",
        "    for line in f:\n",
        "        read_data.append(json.loads(line))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxl4cUCNvsKA"
      },
      "outputs": [],
      "source": [
        "evaluation_systemPrompt = \"\"\"\n",
        "You are an impartial judge. Evaluate the \"Generated Answer\" based on the \"Context\" and \"Ground Truth\".\n",
        "1. Faithfulness: Is the answer derived ONLY from the context?\n",
        "2. Accuracy: Does it match the meaning of the Ground Truth?\n",
        "\n",
        "Return your evaluation in this format:\n",
        "Score: [0 to 10]\n",
        "Reasoning: [Short explanation]\n",
        "\"\"\"\n",
        "\n",
        "def evaluate_fixed():\n",
        "    global read_data, history\n",
        "\n",
        "    results = []\n",
        "    for i in range(5):\n",
        "        print(f\"--- Evaluating Sample {i+1} ---\")\n",
        "\n",
        "        question = read_data[i][\"question\"]\n",
        "        answer = read_data[i][\"ground_truth\"]\n",
        "        context = read_data[i][\"reference_context\"]\n",
        "        history = []\n",
        "\n",
        "        generated_answer = rag(question)\n",
        "\n",
        "        user_prompt = (\n",
        "            f\"Question: {question}\\n\"\n",
        "            f\"Ground Truth: {answer}\\n\"\n",
        "            f\"Context: {context}\\n\"\n",
        "            f\"Generated Answer: {generated_answer}\"\n",
        "        )\n",
        "\n",
        "        message = [\n",
        "            {\"role\": \"system\", \"content\": evaluation_systemPrompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ]\n",
        "\n",
        "        prompt_ids = tokenizer.apply_chat_template(\n",
        "            message,\n",
        "            tokenize=True,\n",
        "            add_generation_prompt=True,\n",
        "            return_dict = True,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(\"cuda\")\n",
        "\n",
        "        output_ids = model.generate(\n",
        "            prompt_ids['input_ids'],\n",
        "            do_sample=False,\n",
        "            attention_mask=prompt_ids['attention_mask'],\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            max_new_tokens=200\n",
        "        )\n",
        "\n",
        "        generated_text = output_ids[0][prompt_ids['input_ids'].shape[1]:]\n",
        "        evaluation_text = tokenizer.decode(generated_text, skip_special_tokens=True)\n",
        "\n",
        "        print(f\"Evaluation:\\n{evaluation_text}\\n\")\n",
        "        results.append(evaluation_text)\n",
        "\n",
        "    return results\n",
        "\n",
        "eval_logs = evaluate_fixed()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_fUMqHTzfqn"
      },
      "outputs": [],
      "source": [
        "print(f\"{'Sample':<10} | {'Score':<5} | {'Summary'}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for i, report in enumerate(eval_logs):\n",
        "\n",
        "    score_match = re.search(r\"Score:\\s*(\\d+)\", report)\n",
        "    score = score_match.group(1) if score_match else \"N/A\"\n",
        "\n",
        "    reasoning_match = re.search(r\"Reasoning:\\s*(.*)\", report)\n",
        "    reasoning = reasoning_match.group(1)[:60] + \"...\" if reasoning_match else \"No reasoning\"\n",
        "\n",
        "    print(f\"Sample {i+1:<3} | {score:<5} | {reasoning}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0e9c4a5c6c36482b8d2e4d64e4ad8152": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1cd1a23236494d02b8a632c3f3b7fed9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9698c0ba2ea2496797430af644a12d07",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a4117d3692474dd98415ae6238312956",
            "value": 2
          }
        },
        "3002282a6b224bfc94260d5e2d5e3ace": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_df33a3d58a4e45189dfbfc0f424164ca",
              "IPY_MODEL_1cd1a23236494d02b8a632c3f3b7fed9",
              "IPY_MODEL_31e8f0e7adcb445599448b84f2a2f257"
            ],
            "layout": "IPY_MODEL_8a69fa38eae443c9a81dd1383e58fe47"
          }
        },
        "31e8f0e7adcb445599448b84f2a2f257": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e9c4a5c6c36482b8d2e4d64e4ad8152",
            "placeholder": "​",
            "style": "IPY_MODEL_d961dcb7390a448380cbf203614bcccb",
            "value": " 2/2 [00:40&lt;00:00, 18.14s/it]"
          }
        },
        "5037b8a97f284f40a77efa8d456200e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8a69fa38eae443c9a81dd1383e58fe47": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9698c0ba2ea2496797430af644a12d07": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4117d3692474dd98415ae6238312956": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d961dcb7390a448380cbf203614bcccb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "df33a3d58a4e45189dfbfc0f424164ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_edde3052bdbb47a98740f68267c454ad",
            "placeholder": "​",
            "style": "IPY_MODEL_5037b8a97f284f40a77efa8d456200e0",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "edde3052bdbb47a98740f68267c454ad": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
